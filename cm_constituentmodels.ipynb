{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1098f7cb",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 constituent models \n",
    "## FCDO project, cm level\n",
    "\n",
    "\n",
    "This notebook trains a set of regression models for use in a predicting fatalities ensemble\n",
    "\n",
    "New version May 2022 using a model list that can be stored as csv rather than a pickle\n",
    "\n",
    "The notebook does the following: \n",
    "1. Retrieves data through querysets and stores in DataSets, a list of dictionaries\n",
    "2. Specifies the metadata of a number of models, stores in ModelList, a list of dictionaries\n",
    "3. Trains the models in ModelList, stores the trained objects in model storage and prediction storage\n",
    "4. Saves part of ModelList as csv and the rest as pickles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f7cba",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef27dd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/havardhegre/anaconda3/envs/viewser/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFRegressor, XGBRFClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "import views_dataviz\n",
    "from views_runs import storage\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "\n",
    "from views_forecasts.extensions import *\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "# Packages from Predicting Fatalies repository\n",
    "\n",
    "#from HurdleRegression import * # Built on script from Geoff Hurdock: https://geoffruddock.com/building-a-hurdle-regression-estimator-in-scikit-learn/\n",
    "from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "from FetchData import FetchData, RetrieveFromList\n",
    "from ViewsEstimators import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300ea25",
   "metadata": {},
   "source": [
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c76adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda list | grep views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fdc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# find out why and where missingness occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ae8aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "# Common parameters:\n",
    "dev_id = 'Fatalities001'\n",
    "run_id = 'Fatalities001'\n",
    "\n",
    "# Generating a new run if necessary\n",
    "\n",
    "try:\n",
    "    ViewsMetadata().new_run(name=run_id,description='Developing the fatalities model for FCDO',min_month=1,max_month=999)\n",
    "except KeyError:\n",
    "    if 'devel' not in run_id:\n",
    "        print('...')\n",
    "#        warnings.warn('You are overwriting a production system')\n",
    "\n",
    "RerunQuerysets = True\n",
    "\n",
    "FutureStart = 506\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,3,6,12,36]\n",
    "#fi_steps = [1,3,6,12,36]\n",
    "\n",
    "# Specifying partitions\n",
    "calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,444)}\n",
    "test_partitioner_dict = {\"train\":(121,444),\"predict\":(445,492)}\n",
    "future_partitioner_dict = {\"train\":(121,492),\"predict\":(493,504)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf0208",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d537fc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .    A dataset with 6 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 58 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 29 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 29 columns, with data between t 1 and 852. (213 units)\n",
      " .      o      O      O      o       .      o   A dataset with 111 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 63 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 54 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 32 columns, with data between t 1 and 852. (213 units)\n",
      " .      o   A dataset with 64 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 39 columns, with data between t 1 and 852. (213 units)\n",
      " .      o      O      O      o       .      o      O      O      o       .    A dataset with 46 columns, with data between t 1 and 852. (213 units)\n",
      " .      o      O      O      o       .      o      O      O      oA dataset with 36 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 181 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 110 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 76 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 59 columns, with data between t 1 and 852. (213 units)\n",
      " .    A dataset with 39 columns, with data between t 1 and 852. (213 units)\n",
      " .      o      O      O      o      A dataset with 17 columns, with data between t 1 and 852. (213 units)\n",
      " .      o      O      O      o       .      o      O      O      o       .      o   A dataset with 41 columns, with data between t 1 and 852. (213 units)\n",
      " .      o      O      O      o       .      o   A dataset with 18 columns, with data between t 1 and 852. (213 units)\n",
      " .      o      O  A dataset with 11 columns, with data between t 1 and 852. (213 units)\n"
     ]
    }
   ],
   "source": [
    "# Specifying querysets\n",
    "# Rerun if querysets have \n",
    "if RerunQuerysets:\n",
    "    import cm_querysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25993909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data using querysets; returns as list of dictionaries containing datasets\n",
      " .    baseline: A dataset with 6 columns, with data between t = 1 and 852; 213 units.\n",
      " .    conflictlong_ln: A dataset with 58 columns, with data between t = 1 and 852; 213 units.\n",
      " .    conflict_ln: A dataset with 29 columns, with data between t = 1 and 852; 213 units.\n",
      " .    conflict_nolog: A dataset with 29 columns, with data between t = 1 and 852; 213 units.\n",
      " .    wdi_short: A dataset with 32 columns, with data between t = 1 and 852; 213 units.\n",
      " .    vdem_short: A dataset with 63 columns, with data between t = 1 and 852; 213 units.\n",
      " .    topics_short: A dataset with 39 columns, with data between t = 1 and 852; 213 units.\n",
      " .    broad: A dataset with 110 columns, with data between t = 1 and 852; 213 units.\n",
      " .    gh: A dataset with 59 columns, with data between t = 1 and 852; 213 units.\n",
      " .    hh20: A dataset with 39 columns, with data between t = 1 and 852; 213 units.\n",
      " .    all_features: A dataset with 181 columns, with data between t = 1 and 852; 213 units.\n",
      "all features [9.23882586e-01 7.58654816e-02 2.46787848e-04 5.12533139e-06\n",
      " 1.87790578e-08 5.73738599e-12 4.88280534e-16 3.47754772e-16\n",
      " 9.56238619e-17 3.91291765e-17 1.18687276e-17 7.45513217e-18\n",
      " 4.74745046e-18 2.08498852e-18 7.85503151e-20 7.63834025e-24\n",
      " 3.37966989e-24 6.48698219e-25 5.37674775e-25 3.72220696e-25]\n",
      "[3.29568887e+16 9.44408580e+15 5.38641362e+14 7.76244954e+13\n",
      " 4.69866750e+12 8.21286811e+10 7.57656790e+08 6.39402550e+08\n",
      " 3.35290273e+08 2.14480675e+08 1.18124476e+08 9.36193212e+07\n",
      " 7.47081591e+07 4.95096263e+07 9.60974315e+06 9.47626756e+04\n",
      " 6.30340128e+04 2.76158978e+04 2.51418740e+04 2.09188629e+04]\n",
      "topics [1.00000000e+00 2.04512547e-13 7.82195221e-15 2.01331791e-16\n",
      " 7.70710443e-17 5.70584094e-18 4.05290453e-18 2.85264699e-18\n",
      " 1.88419526e-18 1.62725629e-18]\n",
      "[3.52759832e+10 1.59528803e+04 3.11987149e+03 5.00535985e+02\n",
      " 3.09688265e+02 8.42633958e+01 7.10169985e+01 5.95803627e+01\n",
      " 4.84219283e+01 4.49994397e+01]\n",
      "vdem [9.99977132e-01 2.28676239e-05 5.46134098e-14 8.44026978e-16\n",
      " 3.07386456e-16 1.82153534e-16 1.23529527e-16 5.45713359e-17\n",
      " 4.18006761e-17 3.38588666e-17 1.69804859e-17 1.06971265e-17\n",
      " 7.49469863e-18 6.87601606e-18 5.32402356e-18]\n",
      "[3.52759963e+10 1.68692111e+08 8.24392250e+03 1.02485499e+03\n",
      " 6.18481363e+02 4.76105342e+02 3.92075444e+02 2.60595281e+02\n",
      " 2.28074067e+02 2.05267627e+02 1.45364819e+02 1.15376622e+02\n",
      " 9.65742493e+01 9.25023349e+01 8.13962202e+01]\n",
      "wdi [9.99645371e-01 3.54310984e-04 3.18300567e-07 1.40457864e-15\n",
      " 3.62588759e-16 1.46754819e-16 1.22762719e-16 4.31581613e-17\n",
      " 2.75400218e-17 2.67708523e-23 1.86787698e-24 7.10241616e-25\n",
      " 9.14510202e-26 3.06443844e-26 2.84318045e-26]\n",
      "[1.77979953e+16 3.35073757e+14 1.00430775e+13 6.67146386e+08\n",
      " 3.38965326e+08 2.15647433e+08 1.97233807e+08 1.16944439e+08\n",
      " 9.34179316e+07 9.21041516e+04 2.43288870e+04 1.50020692e+04\n",
      " 5.38322317e+03 3.11618733e+03 3.00158274e+03]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Datasets = FetchData(dev_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925bdb3",
   "metadata": {},
   "source": [
    "# Generating predictions\n",
    "Using the new partitioning/stepshifting syntax. Training models for A: calibration partition and B: test partition, to test out some calibration routines. Most models trained with ln_ged_sb_best as outcome, but also one model with ged_sb_best to see whether that improves calibration on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b379f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ln_ged_sb_dep</th>\n",
       "      <th>ln_ged_sb</th>\n",
       "      <th>wdi_sp_pop_totl</th>\n",
       "      <th>decay_ged_sb_5</th>\n",
       "      <th>decay_ged_os_5</th>\n",
       "      <th>splag_1_decay_ged_sb_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_id</th>\n",
       "      <th>country_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>359531.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1084744.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15182611.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155525.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ln_ged_sb_dep  ln_ged_sb  wdi_sp_pop_totl  \\\n",
       "month_id country_id                                              \n",
       "1        1                     0.0        0.0         780153.0   \n",
       "         2                     0.0        0.0         359531.0   \n",
       "         3                     0.0        0.0        1084744.0   \n",
       "         4                     0.0        0.0       15182611.0   \n",
       "         5                     0.0        0.0         155525.0   \n",
       "\n",
       "                     decay_ged_sb_5  decay_ged_os_5  splag_1_decay_ged_sb_5  \n",
       "month_id country_id                                                          \n",
       "1        1                      0.0             0.0                     0.0  \n",
       "         2                      0.0             0.0                     0.0  \n",
       "         3                      0.0             0.0                     0.0  \n",
       "         4                      0.0             0.0                     0.0  \n",
       "         5                      0.0             0.0                     0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Datasets[0]['df'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990574dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ModelMetadata in module views_schema.models:\n",
      "\n",
      "class ModelMetadata(pydantic.main.BaseModel)\n",
      " |  ModelMetadata(*, author: str, queryset_name: str, train_start: int, train_end: int, steps: List[int] = None, training_date: datetime.datetime) -> None\n",
      " |  \n",
      " |  ModelMetadata\n",
      " |  =============\n",
      " |  \n",
      " |  Data used to organize model objects.\n",
      " |  \n",
      " |  parameters:\n",
      " |      author (str): Name of the user that authored the model object.\n",
      " |      queryset_name (str): Name of the queryset used to train the model\n",
      " |      train_start (int): Month identifier for training start date\n",
      " |      train_start (int): Month identifier for training end date\n",
      " |      training_date (datetime.datetime): Timestamp for training date (use datetime.datetime.now())\n",
      " |  \n",
      " |  example:\n",
      " |  \n",
      " |      # Instantiate the class with values\n",
      " |  \n",
      " |      my_metadata = ModelMetadata(\n",
      " |          author = \"my_name\",\n",
      " |          queryset_name = \"my_queryset\",\n",
      " |          train_start = 1,\n",
      " |          train_end = 300,\n",
      " |          steps = [1,2,3],\n",
      " |          training_date = datetime.datetime.now())\n",
      " |  \n",
      " |      # Create metadata with a views_runs.ViewsRun object. This fetches\n",
      " |      # values from the associated StepshiftedModels and DataPartitioner\n",
      " |      # objects.\n",
      " |  \n",
      " |      my_metadata = my_run.create_model_metadata(\n",
      " |              author = \"me\",\n",
      " |              queryset_name = \"my_queryset\",\n",
      " |              training_partition_name = \"A\",\n",
      " |              )\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ModelMetadata\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'author': <class 'str'>, 'queryset_name': <class 's...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'views_schema.models.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = None\n",
      " |  \n",
      " |  __fields__ = {'author': ModelField(name='author', type=str, required=T...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = []\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, author: str, queryset_name: str, ... No...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, update: 'DictStrAny' = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, by_alias: bool = False, skip_defaults: bool = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny')] = None, by_alias: bool = False, skip_defaults: bool = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__() -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  Config = <class 'pydantic.config.BaseConfig'>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from views_runs import ModelMetadata \n",
    "help(ModelMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf49bd2",
   "metadata": {},
   "source": [
    "## Checking missingness and infinity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfe61e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_ged_sb_dep 158230 missing: 0 infinity: 0\n",
      "ln_ged_sb 158230 missing: 0 infinity: 0\n",
      "wdi_sp_pop_totl 158230 missing: 11 infinity: 0\n",
      "decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_5 158230 missing: 0 infinity: 0\n",
      "splag_1_decay_ged_sb_5 158230 missing: 0 infinity: 0\n"
     ]
    }
   ],
   "source": [
    "N=51\n",
    "df = Datasets[0]['df']\n",
    "for col in df.iloc[: , :N].columns:\n",
    "    print(col,len(df[col]), 'missing:', df[col].isnull().sum(), 'infinity:', np.isinf(df).values.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c65378",
   "metadata": {},
   "source": [
    "## Identify early stopping parameter\n",
    "\n",
    "See the Early_stopping_experiment notebook for how we arrived at the early stopping parameters for the XGBoost models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761eb9c",
   "metadata": {},
   "source": [
    "# Specify models in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d714b499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fat_baseline_rf baseline\n",
      "1 fat_conflicthistory_srf conflict_ln\n",
      "2 fat_conflicthistory_rf conflict_ln\n",
      "3 fat_conflicthistory_gbm conflict_ln\n",
      "4 fat_conflicthistory_hurdle_rf conflict_ln\n",
      "5 fat_conflicthistory_hurdle_xgb conflict_ln\n",
      "6 fat_conflicthistory_hurdle_lgb conflict_ln\n",
      "7 fat_conflicthistory_histgbm conflict_ln\n",
      "8 fat_conflicthistory_xgb conflict_ln\n",
      "9 fat_conflicthistory_lgb conflict_ln\n",
      "10 fat_conflicthistory_long_rf conflictlong_ln\n",
      "11 fat_conflicthistory_long_xgb conflictlong_ln\n",
      "12 fat_vdem_rf vdem_short\n",
      "13 fat_vdem_xgb vdem_short\n",
      "14 fat_vdem_hurdle_xgb vdem_short\n",
      "15 fat_wdi_rf wdi_short\n",
      "16 fat_wdi_xgb wdi_short\n",
      "17 fat_wdi_hurdle_xgb wdi_short\n",
      "18 fat_topics_rf topics_short\n",
      "19 fat_topics_histgbm topics_short\n",
      "20 fat_topics_xgb topics_short\n",
      "21 fat_topics_hurdle_xgb topics_short\n",
      "22 fat_prs_rf prs\n",
      "23 fat_prs_xgb prs\n",
      "24 fat_prs_hurdle_xgb prs\n",
      "25 fat_broad_rf broad\n",
      "26 fat_broad_xgb broad\n",
      "27 fat_broad_hurdle_xgb broad\n",
      "28 fat_greatest_hits_rf gh\n",
      "29 fat_greatest_hits_xgb gh\n",
      "30 fat_greatest_hits_hurdle_xgb gh\n",
      "31 fat_greatest_hits_lgb gh\n",
      "32 fat_hh20_srf hh20\n",
      "33 fat_hh20_rf hh20\n",
      "34 fat_hh20_gbm hh20\n",
      "35 fat_hh20_hurdle_rf hh20\n",
      "36 fat_hh20_hurdle_xgb hh20\n",
      "37 fat_hh20_hurdle_lgb hh20\n",
      "38 fat_hh20_histgbm hh20\n",
      "39 fat_hh20_xgb hh20\n",
      "40 fat_hh20_lgb hh20\n",
      "41 fat_all_pca3_xgb pca_all\n",
      "42 fat_all_pca3_lgb pca_all\n",
      "43 fat_topics_pca3_xgb pca_topics\n",
      "44 fat_topics_pca3_lgb pca_topics\n",
      "45 fat_vdem_pca3_lgb pca_vdem\n",
      "46 fat_wdi_pca3_lgb pca_wdi\n"
     ]
    }
   ],
   "source": [
    "# The ModelList is a list of dictionaries that define a range of models for the project\n",
    "\n",
    "# To do: Include the preprocessing function as item in dictionary, to be able to run the PCA models.\n",
    "\n",
    "ModelList = []\n",
    "nj=12\n",
    "\n",
    "model = {\n",
    "    'modelname':     'fat_baseline_rf',\n",
    "    'algorithm':     XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar':        \"ln_ged_sb_dep\",\n",
    "    'data_train':    'baseline',\n",
    "    'queryset':      'hh_fatalities_ged_ln_ultrashort',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_srf',\n",
    "    'algorithm': RandomForestRegressor(n_estimators=200, n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "# Model: GED logged dependent variable, logged conflict history variables, gradient boosting\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_gbm',\n",
    "    'algorithm': GradientBoostingRegressor(), \n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)       \n",
    "    \n",
    "# Conflict history hurdle\n",
    " # Model: GED logged dependent variable, hurdle/random forest with default settings\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_hurdle_rf',\n",
    "    'algorithm': HurdleRegression(clf_name = 'RFClassifier', reg_name = 'RFRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_hurdle_lgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'LGBMClassifier', reg_name = 'LGBMRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_histgbm',\n",
    "    'algorithm': HistGradientBoostingRegressor(max_iter=200),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_xgb_gamma',\n",
    "    'algorithm': XGBRegressor(n_estimators=200,tree_method='hist', objective='reg:gamma',n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "#ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_xgb_poisson',\n",
    "    'algorithm': XGBRegressor(n_estimators=200,tree_method='hist', objective='count:poisson',n_jobs=nj),\n",
    "    'depvar': \"ged_sb_dep\",\n",
    "    'data_train':    'conflict_nolog',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "#ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_lgb',\n",
    "    'algorithm': LGBMRegressor(n_estimators=100,num_threads=8),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_long_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflictlong_ln',\n",
    "    'queryset':  \"hh_fatalities_ged_acled_ln\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_long_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflictlong_ln',\n",
    "    'queryset': \"hh_fatalities_ged_acled_ln\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_vdem_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'vdem_short',\n",
    "    'queryset': \"hh_fatalities_vdem_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_vdem_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'vdem_short',\n",
    "    'queryset':  \"hh_fatalities_vdem_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_vdem_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'vdem_short',\n",
    "    'queryset':  \"hh_fatalities_vdem_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_wdi_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'wdi_short',\n",
    "    'queryset':  \"hh_fatalities_wdi_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_wdi_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'wdi_short',\n",
    "    'queryset': \"hh_fatalities_wdi_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_wdi_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'wdi_short',\n",
    "    'queryset':  \"hh_fatalities_wdi_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'topics_short',\n",
    "    'queryset':   \"hh_topic_model_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_histgbm',\n",
    "    'algorithm': HistGradientBoostingRegressor(max_iter=200),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'topics_short',\n",
    "    'queryset':   \"hh_topic_model_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_xgb',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'topics_short',\n",
    "    'queryset':   \"hh_topic_model_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'topics_short',\n",
    "    'queryset':   \"hh_topic_model_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_prs_rf',\n",
    "    'algorithm':  XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'prs',\n",
    "    'queryset':   'hh_prs',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_prs_xgb',\n",
    "    'algorithm':  XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'prs',\n",
    "    'queryset':   'hh_prs',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_prs_hurdle_xgb',\n",
    "    'algorithm':  HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'prs',\n",
    "    'queryset':   'hh_prs',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_broad_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'broad',\n",
    "    'queryset':   'hh_broad',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_broad_xgb',\n",
    "    'algorithm':  XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'broad',\n",
    "    'queryset':   'hh_broad',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_broad_hurdle_xgb',\n",
    "    'algorithm':  HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'broad',\n",
    "    'queryset':   'hh_broad',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_greatest_hits_rf',\n",
    "    'algorithm':  XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'gh',\n",
    "    'queryset':   'hh_greatest_hits',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_greatest_hits_xgb',\n",
    "    'algorithm':  XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'gh',\n",
    "    'queryset':   'hh_greatest_hits',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_greatest_hits_hurdle_xgb',\n",
    "    'algorithm':  HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'gh',\n",
    "    'queryset':   'hh_greatest_hits',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_greatest_hits_lgb',\n",
    "    'algorithm':  LGBMRegressor(n_estimators=100,num_threads=8),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'gh',\n",
    "    'queryset':   'hh_greatest_hits',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_srf',\n",
    "    'algorithm': RandomForestRegressor(n_estimators=200, n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_gbm',\n",
    "    'algorithm': GradientBoostingRegressor(), \n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)       \n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_hurdle_rf',\n",
    "    'algorithm': HurdleRegression(clf_name = 'RFClassifier', reg_name = 'RFRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_hurdle_lgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'LGBMClassifier', reg_name = 'LGBMRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_histgbm',\n",
    "    'algorithm': HistGradientBoostingRegressor(max_iter=200),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_hh20_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname' :      'fat_hh20_lgb',\n",
    "    'algorithm' :      LGBMRegressor(n_estimators=200,num_threads=8),\n",
    "    'depvar':          \"ln_ged_sb_dep\",\n",
    "    'data_train':      'hh20',\n",
    "    'queryset':        'hh_20_features',\n",
    "    'preprocessing':   'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "# PCA models: need to implement a PCA preprocessing function first.\n",
    "model = {\n",
    "    'modelname':      'fat_all_pca3_xgb',\n",
    "    'algorithm':      XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar':         \"ln_ged_sb_dep\",\n",
    "    'data_train':     'pca_all',\n",
    "    'queryset':      'hh_all_features',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "# PCA models: need to implement a PCA preprocessing function first.\n",
    "model = {\n",
    "    'modelname':  'fat_all_pca3_lgb',\n",
    "    'algorithm': LGBMRegressor(n_estimators=100,num_threads=12),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'pca_all',\n",
    "    'queryset': 'hh_all_features',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_pca3_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'pca_topics',\n",
    "    'queryset': 'hh_topics_short',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_pca3_lgb',\n",
    "    'algorithm': LGBMRegressor(n_estimators=200,num_threads=12),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'pca_topics',\n",
    "    'queryset': 'hh_topics_short',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_vdem_pca3_lgb',\n",
    "    'algorithm': LGBMRegressor(n_estimators=200,num_threads=12),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'pca_vdem',\n",
    "    'queryset': 'hh_vdem_short',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':      'fat_wdi_pca3_lgb',\n",
    "    'algorithm':      LGBMRegressor(n_estimators=200,num_threads=12),\n",
    "    'depvar':         \"ln_ged_sb_dep\",\n",
    "    'data_train':     'pca_wdi',\n",
    "    'queryset':       'hh_wdi_short',\n",
    "    'preprocessing':  'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "i = 0\n",
    "for model in ModelList:\n",
    "    print(i, model['modelname'], model['data_train'])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10c58f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fat_greatest_hits_xgb\n",
      "Calibration partition 2022-06-20 18:43:28.225802\n",
      " * == Performing a run: \"fat_greatest_hits_xgb_calib\" == * \n",
      "Model object named \"fat_greatest_hits_xgb_calib\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_xgb_calib\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:43:30.641449\n",
      "pr_45_cm_fat_greatest_hits_xgb_calib.parquet\n",
      "Test partition 2022-06-20 18:43:34.658779\n",
      " * == Performing a run: \"fat_greatest_hits_xgb_test\" == * \n",
      "Model object named \"fat_greatest_hits_xgb_test\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_xgb_test\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:43:37.991294\n",
      "pr_45_cm_fat_greatest_hits_xgb_test.parquet\n",
      "Future 2022-06-20 18:43:41.093545\n",
      " * == Performing a run: \"fat_greatest_hits_xgb_future\" == * \n",
      "Model object named \"fat_greatest_hits_xgb_future\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_xgb_future\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:43:44.042860\n",
      "pr_45_cm_fat_greatest_hits_xgb_f506.parquet\n",
      "1 fat_greatest_hits_hurdle_xgb\n",
      "Calibration partition 2022-06-20 18:43:47.496516\n",
      " * == Performing a run: \"fat_greatest_hits_hurdle_xgb_calib\" == * \n",
      "Model object named \"fat_greatest_hits_hurdle_xgb_calib\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_hurdle_xgb_calib\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:43:51.546846\n",
      "pr_45_cm_fat_greatest_hits_hurdle_xgb_calib.parquet\n",
      "Test partition 2022-06-20 18:43:54.818457\n",
      " * == Performing a run: \"fat_greatest_hits_hurdle_xgb_test\" == * \n",
      "Model object named \"fat_greatest_hits_hurdle_xgb_test\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_hurdle_xgb_test\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:43:59.511443\n",
      "pr_45_cm_fat_greatest_hits_hurdle_xgb_test.parquet\n",
      "Future 2022-06-20 18:44:02.585068\n",
      " * == Performing a run: \"fat_greatest_hits_hurdle_xgb_future\" == * \n",
      "Model object named \"fat_greatest_hits_hurdle_xgb_future\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_hurdle_xgb_future\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:44:07.741565\n",
      "pr_45_cm_fat_greatest_hits_hurdle_xgb_f506.parquet\n",
      "2 fat_greatest_hits_lgb\n",
      "Calibration partition 2022-06-20 18:44:10.610063\n",
      " * == Performing a run: \"fat_greatest_hits_lgb_calib\" == * \n",
      "Model object named \"fat_greatest_hits_lgb_calib\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_lgb_calib\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:44:11.361165\n",
      "pr_45_cm_fat_greatest_hits_lgb_calib.parquet\n",
      "Test partition 2022-06-20 18:44:14.899458\n",
      " * == Performing a run: \"fat_greatest_hits_lgb_test\" == * \n",
      "Model object named \"fat_greatest_hits_lgb_test\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_lgb_test\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:44:15.790850\n",
      "pr_45_cm_fat_greatest_hits_lgb_test.parquet\n",
      "Future 2022-06-20 18:44:18.700600\n",
      " * == Performing a run: \"fat_greatest_hits_lgb_future\" == * \n",
      "Model object named \"fat_greatest_hits_lgb_future\" with equivalent metadata already exists.\n",
      "Fetching \"fat_greatest_hits_lgb_future\" from storage\n",
      "Trying to retrieve predictions 2022-06-20 18:44:19.586213\n",
      "pr_45_cm_fat_greatest_hits_lgb_f506.parquet\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "# Loop that checks whether the model exists, retrains if not, \n",
    "# and stores the predictions if they have not been stored before for this run.\n",
    "# To do: set the data_preprocessing to the function in the model dictionary\n",
    "\n",
    "level = 'cm'\n",
    "includeFuture = True\n",
    "\n",
    "from views_runs import Storage, StepshiftedModels\n",
    "from views_partitioning.data_partitioner import DataPartitioner\n",
    "from viewser import Queryset, Column\n",
    "from views_runs import operations\n",
    "from views_runs.run_result import RunResult\n",
    "\n",
    "i = 0\n",
    "for model in ModelList[29:32]:\n",
    "    force_retrain = False\n",
    "    modelstore = storage.Storage()\n",
    "    ct = datetime.now()\n",
    "    print(i, model['modelname'])\n",
    "    print('Calibration partition', ct)\n",
    "    model['Algorithm_text'] = str(model['algorithm'])\n",
    "    model['RunResult_calib'] = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"calib\":calib_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"calib\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_calib',\n",
    "            author_name        = \"HH\",\n",
    "    )\n",
    "\n",
    "    model['predstore_calib'] = level +  '_' + model['modelname'] + '_calib'\n",
    "    ct = datetime.now()\n",
    "    print('Trying to retrieve predictions', ct)\n",
    "    try:\n",
    "        predictions_calib = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_calib'])\n",
    "    except KeyError:\n",
    "        print(model['predstore_calib'], ', run',  run_id, 'does not exist, predicting')\n",
    "        predictions_calib = model['RunResult_calib'].run.predict(\"calib\",\"predict\", model['RunResult_calib'].data)\n",
    "        predictions_calib.forecasts.set_run(run_id)\n",
    "        predictions_calib.forecasts.to_store(name=model['predstore_calib'])\n",
    "\n",
    "    ct = datetime.now()\n",
    "    print('Test partition', ct)\n",
    "    modelstore = storage.Storage()\n",
    "    model['RunResult_test'] = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"test\":test_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"test\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_test',\n",
    "            author_name        = \"HH\",\n",
    "    )\n",
    "    ct = datetime.now()\n",
    "    print('Trying to retrieve predictions', ct)\n",
    "    model['predstore_test'] = level +  '_' + model['modelname'] + '_test'\n",
    "    try:\n",
    "        predictions_test = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_test'])\n",
    "    except KeyError:\n",
    "        print(model['predstore_test'], ', run', run_id, 'does not exist, predicting')\n",
    "        predictions_test = model['RunResult_test'].run.predict(\"test\",\"predict\",model['RunResult_test'].data)\n",
    "        predictions_test.forecasts.set_run(run_id)\n",
    "        predictions_test.forecasts.to_store(name=model['predstore_test'])\n",
    "    # Predictions for true future\n",
    "    if includeFuture:\n",
    "        ct = datetime.now()\n",
    "        print('Future', ct)\n",
    "        modelstore = storage.Storage()\n",
    "        model['RunResult_future'] = RunResult.retrain_or_retrieve(\n",
    "                retrain            = force_retrain,\n",
    "                store              = modelstore,\n",
    "                partitioner        = DataPartitioner({\"test\":future_partitioner_dict}),\n",
    "                stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                queryset_name      = model['queryset'],\n",
    "                partition_name     = \"test\",\n",
    "                timespan_name      = \"train\",\n",
    "                storage_name       = model['modelname'] + '_future',\n",
    "                author_name        = \"HH\",\n",
    "        )\n",
    "        ct = datetime.now()\n",
    "        print('Trying to retrieve predictions', ct)\n",
    "        model['predstore_future'] = level +  '_' + model['modelname'] + '_f' + str(FutureStart)\n",
    "        try:\n",
    "            predictions_future = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_future'])\n",
    "        except KeyError:\n",
    "            print(model['predstore_future'], ', run', run_id, 'does not exist, predicting')\n",
    "            predictions_future = model['RunResult_future'].run.future_point_predict(FutureStart,model['RunResult_future'].data)\n",
    "            predictions_future.forecasts.set_run(run_id)\n",
    "            predictions_future.forecasts.to_store(name=model['predstore_future'])  \n",
    "    model['algorithm'] = []\n",
    "    i = i + 1\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bb08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the future predictions\n",
    "\n",
    "\n",
    "predictions_test.xs(246,level=1).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b52249",
   "metadata": {},
   "source": [
    "## Notes on training time for the various algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are calculated in minutes for the hh20 feature set (with about 40 features), for all 36 steps, calibration (c) and test (t) partitions, also include generating predictions, and are approximate:\n",
    "\n",
    "#nj=12 (number of threads)\n",
    "#scikit random forest:        21:13 (c), 26:20 (t) RandomForestRegressor(n_estimators=200, n_jobs=nj)\n",
    "#XGB random forest:           06:02 (c), 07:51 (t) XGBRFRegressor(n_estimators=300,n_jobs=nj)\n",
    "#scikit gbm:                  13:59 (c), 15:55 (t) GradientBoostingRegressor(), \n",
    "#scikit hurdle random forest: 07:32 (c), 09:49 (t) For both clf and reg: (n_estimators=200, n_jobs=nj)\n",
    "#XGB hurdle xgb:              01:26 (c), 01:32 (t) For both clf and reg:                n_estimators=200,tree_method='hist',n_jobs=nj)\n",
    "#scikit histgbm:              01:17 (c), 01:20 (t) HistGradientBoostingRegressor(max_iter=200)\n",
    "#XGB xgb:                     01:00 (c), 01:04 (t) XGBRegressor(n_estimators=200,tree_method='hist',n_jobs=nj)\n",
    "#lightgbm gbm:                00:25 (c), --    (t) LGBMRegressor(n_estimators=100,num_threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71483a35",
   "metadata": {},
   "source": [
    "# Various helper functions and tools...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f053fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list | grep views-forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7570c6",
   "metadata": {},
   "source": [
    "# Retrieving external forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30211fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve David's Markov models\n",
    "# To do: rewrite the model dictionary to the new, slimmer version.\n",
    "DRList = []\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':   'fat_hh20_Markov_glm',\n",
    "    'algorithm': [],\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':      'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "}\n",
    "ModelList.append(model)\n",
    "DRList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':   'fat_hh20_Markov_rf',\n",
    "    'algorithm': [],\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':      'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "}\n",
    "ModelList.append(model)\n",
    "DRList.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "\n",
    "DRList[0]['predictions_file_calib'] = path + 'vmm_glm_hh20_0125_alt_calib.csv'\n",
    "DRList[0]['predictions_file_test'] = path + 'vmm_glm_hh20_0125_alt_test.csv'\n",
    "DRList[0]['predictions_file_future'] = path + 'vmm_glm_hh20_506.csv'\n",
    "\n",
    "DRList[1]['predictions_file_calib'] = path + 'vmm_rf_hh20_0125_alt_calib.csv'\n",
    "DRList[1]['predictions_file_test'] = path + 'vmm_rf_hh20_0125_alt_test.csv'\n",
    "DRList[1]['predictions_file_future'] = path + 'vmm_rf_hh20_505.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1e5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86478962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Markov models in central storage\n",
    "# Retrieving dependent variable\n",
    "target_calib = pd.DataFrame.forecasts.read_store('cm_fat_conflicthistory_rf_calib', run=run_id)['ln_ged_sb_dep']\n",
    "target_test = pd.DataFrame.forecasts.read_store('cm_fat_conflicthistory_rf_test', run=run_id)['ln_ged_sb_dep']\n",
    "level = 'cm'\n",
    "for model in DRList:\n",
    "    df_calib = pd.read_csv(model['predictions_file_calib'],index_col=['month_id','country_id'])\n",
    "    df_test = pd.read_csv(model['predictions_file_test'],index_col=['month_id','country_id'])\n",
    "    df_future = pd.read_csv(model['predictions_file_future'],index_col=['month_id','country_id'])\n",
    "    df_calib['ln_ged_sb_dep'] = target_calib\n",
    "    df_test['ln_ged_sb_dep'] = target_test\n",
    "    df_future['ln_ged_sb_dep'] = np.nan # Empty dependent variable column for consistency/required by prediction storage function\n",
    "    stored_modelname = level + '_' + model['modelname'] + '_calib'\n",
    "    df_calib.forecasts.set_run(run_id)\n",
    "    df_calib.forecasts.to_store(name=stored_modelname, overwrite=True)\n",
    "    stored_modelname = level + '_' + model['modelname'] + '_test'\n",
    "    df_test.forecasts.set_run(run_id)\n",
    "    df_test.forecasts.to_store(name=stored_modelname, overwrite=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing run result objects before saving\n",
    "# These should perhaps be discarded immediately above\n",
    "for model in ModelList[0:-3]:\n",
    "    model['RunResult_calib'] = []\n",
    "    model['RunResult_test'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f47c0f",
   "metadata": {},
   "source": [
    "# Save the model list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7493dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelList_df = pd.DataFrame.from_dict(ModelList)\n",
    "localpath = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/'\n",
    "\n",
    "filename = localpath + 'Model_cm_' + model['modelname'] + '_'+ dev_id + '.csv'\n",
    "ModelList_df.to_csv(filename)\n",
    "gitname = 'ModelList_cm_wide_' + dev_id + '.csv'\n",
    "ModelList_df.to_csv(gitname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
