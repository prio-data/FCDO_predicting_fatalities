{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1098f7cb",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 constituent models \n",
    "## FCDO project, cm level\n",
    "\n",
    "\n",
    "This notebook trains a set of regression models for use in a predicting fatalities ensemble\n",
    "\n",
    "New version May 2022 using a model list that can be stored as csv rather than a pickle\n",
    "\n",
    "The notebook does the following: \n",
    "1. Retrieves data through querysets and stores in DataSets, a list of dictionaries\n",
    "2. Specifies the metadata of a number of models, stores in ModelList, a list of dictionaries\n",
    "3. Trains the models in ModelList, stores the trained objects in model storage and prediction storage\n",
    "4. Saves part of ModelList as csv and the rest as pickles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f7cba",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFRegressor, XGBRFClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "import views_dataviz\n",
    "from views_runs import storage\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "\n",
    "from views_forecasts.extensions import *\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "# Packages from Predicting Fatalies repository\n",
    "\n",
    "#from HurdleRegression import * # Built on script from Geoff Hurdock: https://geoffruddock.com/building-a-hurdle-regression-estimator-in-scikit-learn/\n",
    "from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "from FetchData import FetchData, RetrieveFromList\n",
    "from ViewsEstimators import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300ea25",
   "metadata": {},
   "source": [
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c76adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda list | grep views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# find out why and where missingness occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters:\n",
    "dev_id = 'Fatalities001'\n",
    "run_id = 'Fatalities001'\n",
    "\n",
    "# Generating a new run if necessary\n",
    "\n",
    "try:\n",
    "    ViewsMetadata().new_run(name=run_id,description='Developing the fatalities model for FCDO',min_month=1,max_month=999)\n",
    "except KeyError:\n",
    "    if 'devel' not in run_id:\n",
    "        warnings.warn('You are overwriting a production system')\n",
    "\n",
    "RerunQuerysets = True\n",
    "\n",
    "FutureStart = 506\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,3,6,12,36]\n",
    "#fi_steps = [1,3,6,12,36]\n",
    "\n",
    "# Specifying partitions\n",
    "calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,444)}\n",
    "test_partitioner_dict = {\"train\":(121,444),\"predict\":(445,492)}\n",
    "future_partitioner_dict = {\"train\":(121,492),\"predict\":(493,504)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf0208",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying querysets\n",
    "# Rerun if querysets have \n",
    "if RerunQuerysets:\n",
    "    import cm_querysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25993909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Datasets = FetchData(dev_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925bdb3",
   "metadata": {},
   "source": [
    "# Generating predictions\n",
    "Using the new partitioning/stepshifting syntax. Training models for A: calibration partition and B: test partition, to test out some calibration routines. Most models trained with ln_ged_sb_best as outcome, but also one model with ged_sb_best to see whether that improves calibration on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b379f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets[0]['df'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990574dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from views_runs import ModelMetadata \n",
    "help(ModelMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf49bd2",
   "metadata": {},
   "source": [
    "## Checking missingness and infinity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe61e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=51\n",
    "df = Datasets[0]['df']\n",
    "for col in df.iloc[: , :N].columns:\n",
    "    print(col,len(df[col]), 'missing:', df[col].isnull().sum(), 'infinity:', np.isinf(df).values.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c65378",
   "metadata": {},
   "source": [
    "## Identify early stopping parameter\n",
    "\n",
    "See the Early_stopping_experiment notebook for how we arrived at the early stopping parameters for the XGBoost models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761eb9c",
   "metadata": {},
   "source": [
    "# Specify models in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ModelList is a list of dictionaries that define a range of models for the project\n",
    "\n",
    "# To do: Include the preprocessing function as item in dictionary, to be able to run the PCA models.\n",
    "\n",
    "ModelList = []\n",
    "nj=12\n",
    "\n",
    "model = {\n",
    "    'modelname':     'fat_baseline_rf',\n",
    "    'algorithm':     XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar':        \"ln_ged_sb_dep\",\n",
    "    'data_train':    'baseline',\n",
    "    'queryset':      'hh_fatalities_ged_ln_ultrashort',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_srf',\n",
    "    'algorithm': RandomForestRegressor(n_estimators=200, n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "# Model: GED logged dependent variable, logged conflict history variables, gradient boosting\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_gbm',\n",
    "    'algorithm': GradientBoostingRegressor(), \n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)       \n",
    "    \n",
    "# Conflict history hurdle\n",
    " # Model: GED logged dependent variable, hurdle/random forest with default settings\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_hurdle_rf',\n",
    "    'algorithm': HurdleRegression(clf_name = 'RFClassifier', reg_name = 'RFRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_hurdle_lgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'LGBMClassifier', reg_name = 'LGBMRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_histgbm',\n",
    "    'algorithm': HistGradientBoostingRegressor(max_iter=200),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_xgb_gamma',\n",
    "    'algorithm': XGBRegressor(n_estimators=200,tree_method='hist', objective='reg:gamma',n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "#ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_xgb_poisson',\n",
    "    'algorithm': XGBRegressor(n_estimators=200,tree_method='hist', objective='count:poisson',n_jobs=nj),\n",
    "    'depvar': \"ged_sb_dep\",\n",
    "    'data_train':    'conflict_nolog',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "#ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_lgb',\n",
    "    'algorithm': LGBMRegressor(n_estimators=100,num_threads=8),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflict_ln',\n",
    "    'queryset': \"fat_cm_conflict_history\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_conflicthistory_long_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflictlong_ln',\n",
    "    'queryset':  \"hh_fatalities_ged_acled_ln\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_conflicthistory_long_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'conflictlong_ln',\n",
    "    'queryset': \"hh_fatalities_ged_acled_ln\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_vdem_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'vdem_short',\n",
    "    'queryset': \"hh_fatalities_vdem_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_vdem_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'vdem_short',\n",
    "    'queryset':  \"hh_fatalities_vdem_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_vdem_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'vdem_short',\n",
    "    'queryset':  \"hh_fatalities_vdem_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_wdi_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'wdi_short',\n",
    "    'queryset':  \"hh_fatalities_wdi_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_wdi_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'wdi_short',\n",
    "    'queryset': \"hh_fatalities_wdi_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_wdi_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'wdi_short',\n",
    "    'queryset':  \"hh_fatalities_wdi_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'topics_short',\n",
    "    'queryset':   \"hh_topic_model_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_histgbm',\n",
    "    'algorithm': HistGradientBoostingRegressor(max_iter=200),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'topics_short',\n",
    "    'queryset':   \"hh_topic_model_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_xgb',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'topics_short',\n",
    "    'queryset':   \"hh_topic_model_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'topics_short',\n",
    "    'queryset':   \"hh_topic_model_short\",\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_prs_rf',\n",
    "    'algorithm':  XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'prs',\n",
    "    'queryset':   'hh_prs',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_prs_xgb',\n",
    "    'algorithm':  XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'prs',\n",
    "    'queryset':   'hh_prs',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_prs_hurdle_xgb',\n",
    "    'algorithm':  HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'prs',\n",
    "    'queryset':   'hh_prs',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_broad_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'broad',\n",
    "    'queryset':   'hh_broad',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_broad_xgb',\n",
    "    'algorithm':  XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'broad',\n",
    "    'queryset':   'hh_broad',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_broad_hurdle_xgb',\n",
    "    'algorithm':  HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'broad',\n",
    "    'queryset':   'hh_broad',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_greatest_hits_rf',\n",
    "    'algorithm':  XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'gh',\n",
    "    'queryset':   'hh_greatest_hits',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_greatest_hits_xgb',\n",
    "    'algorithm':  XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'gh',\n",
    "    'queryset':   'hh_greatest_hits',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_greatest_hits_hurdle_xgb',\n",
    "    'algorithm':  HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'gh',\n",
    "    'queryset':   'hh_greatest_hits',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_greatest_hits_lgb',\n",
    "    'algorithm':  LGBMRegressor(n_estimators=100,num_threads=8),\n",
    "    'depvar':     \"ln_ged_sb_dep\",\n",
    "    'data_train':    'gh',\n",
    "    'queryset':   'hh_greatest_hits',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_srf',\n",
    "    'algorithm': RandomForestRegressor(n_estimators=200, n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_rf',\n",
    "    'algorithm': XGBRFRegressor(n_estimators=300,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_gbm',\n",
    "    'algorithm': GradientBoostingRegressor(), \n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)       \n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_hurdle_rf',\n",
    "    'algorithm': HurdleRegression(clf_name = 'RFClassifier', reg_name = 'RFRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_hurdle_xgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'XGBClassifier', reg_name = 'XGBRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_hurdle_lgb',\n",
    "    'algorithm': HurdleRegression(clf_name = 'LGBMClassifier', reg_name = 'LGBMRegressor'),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname': 'fat_hh20_histgbm',\n",
    "    'algorithm': HistGradientBoostingRegressor(max_iter=200),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_hh20_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "    'preprocessing': 'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname' :      'fat_hh20_lgb',\n",
    "    'algorithm' :      LGBMRegressor(n_estimators=200,num_threads=8),\n",
    "    'depvar':          \"ln_ged_sb_dep\",\n",
    "    'data_train':      'hh20',\n",
    "    'queryset':        'hh_20_features',\n",
    "    'preprocessing':   'float_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "# PCA models: need to implement a PCA preprocessing function first.\n",
    "model = {\n",
    "    'modelname':      'fat_all_pca3_xgb',\n",
    "    'algorithm':      XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar':         \"ln_ged_sb_dep\",\n",
    "    'data_train':     'pca_all',\n",
    "    'queryset':      'hh_all_features',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "# PCA models: need to implement a PCA preprocessing function first.\n",
    "model = {\n",
    "    'modelname':  'fat_all_pca3_lgb',\n",
    "    'algorithm': LGBMRegressor(n_estimators=100,num_threads=12),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'pca_all',\n",
    "    'queryset': 'hh_all_features',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_pca3_xgb',\n",
    "    'algorithm': XGBRegressor(n_estimators=100,learning_rate=0.05,n_jobs=nj),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'pca_topics',\n",
    "    'queryset': 'hh_topics_short',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_topics_pca3_lgb',\n",
    "    'algorithm': LGBMRegressor(n_estimators=200,num_threads=12),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'pca_topics',\n",
    "    'queryset': 'hh_topics_short',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':  'fat_vdem_pca3_lgb',\n",
    "    'algorithm': LGBMRegressor(n_estimators=200,num_threads=12),\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':    'pca_vdem',\n",
    "    'queryset': 'hh_vdem_short',\n",
    "    'preprocessing': 'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':      'fat_wdi_pca3_lgb',\n",
    "    'algorithm':      LGBMRegressor(n_estimators=200,num_threads=12),\n",
    "    'depvar':         \"ln_ged_sb_dep\",\n",
    "    'data_train':     'pca_wdi',\n",
    "    'queryset':       'hh_wdi_short',\n",
    "    'preprocessing':  'pca_it',\n",
    "}\n",
    "ModelList.append(model)\n",
    "\n",
    "i = 0\n",
    "for model in ModelList:\n",
    "    print(i, model['modelname'], model['data_train'])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c58f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that checks whether the model exists, retrains if not, \n",
    "# and stores the predictions if they have not been stored before for this run.\n",
    "# To do: set the data_preprocessing to the function in the model dictionary\n",
    "\n",
    "level = 'cm'\n",
    "includeFuture = True\n",
    "\n",
    "from views_runs import Storage, StepshiftedModels\n",
    "from views_partitioning.data_partitioner import DataPartitioner\n",
    "from viewser import Queryset, Column\n",
    "from views_runs import operations\n",
    "from views_runs.run_result import RunResult\n",
    "\n",
    "i = 0\n",
    "for model in ModelList[32:]:\n",
    "    force_retrain = False\n",
    "    modelstore = storage.Storage()\n",
    "    ct = datetime.now()\n",
    "    print(i, model['modelname'])\n",
    "    print('Calibration partition', ct)\n",
    "    model['Algorithm_text'] = str(model['algorithm'])\n",
    "    model['RunResult_calib'] = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"calib\":calib_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"calib\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_calib',\n",
    "            author_name        = \"HH\",\n",
    "    )\n",
    "\n",
    "    model['predstore_calib'] = level +  '_' + model['modelname'] + '_calib'\n",
    "    ct = datetime.now()\n",
    "    print('Trying to retrieve predictions', ct)\n",
    "    try:\n",
    "        predictions_calib = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_calib'])\n",
    "    except KeyError:\n",
    "        print(model['predstore_calib'], ', run',  run_id, 'does not exist, predicting')\n",
    "        predictions_calib = model['RunResult_calib'].run.predict(\"calib\",\"predict\", model['RunResult_calib'].data)\n",
    "        predictions_calib.forecasts.set_run(run_id)\n",
    "        predictions_calib.forecasts.to_store(name=model['predstore_calib'])\n",
    "\n",
    "    ct = datetime.now()\n",
    "    print('Test partition', ct)\n",
    "    modelstore = storage.Storage()\n",
    "    model['RunResult_test'] = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"test\":test_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"test\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_test',\n",
    "            author_name        = \"HH\",\n",
    "    )\n",
    "    ct = datetime.now()\n",
    "    print('Trying to retrieve predictions', ct)\n",
    "    model['predstore_test'] = level +  '_' + model['modelname'] + '_test'\n",
    "    try:\n",
    "        predictions_test = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_test'])\n",
    "    except KeyError:\n",
    "        print(model['predstore_test'], ', run', run_id, 'does not exist, predicting')\n",
    "        predictions_test = model['RunResult_test'].run.predict(\"test\",\"predict\",model['RunResult_test'].data)\n",
    "        predictions_test.forecasts.set_run(run_id)\n",
    "        predictions_test.forecasts.to_store(name=model['predstore_test'])\n",
    "    # Predictions for true future\n",
    "    if includeFuture:\n",
    "        ct = datetime.now()\n",
    "        print('Future', ct)\n",
    "        modelstore = storage.Storage()\n",
    "        model['RunResult_future'] = RunResult.retrain_or_retrieve(\n",
    "                retrain            = force_retrain,\n",
    "                store              = modelstore,\n",
    "                partitioner        = DataPartitioner({\"test\":future_partitioner_dict}),\n",
    "                stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                queryset_name      = model['queryset'],\n",
    "                partition_name     = \"test\",\n",
    "                timespan_name      = \"train\",\n",
    "                storage_name       = model['modelname'] + '_future',\n",
    "                author_name        = \"HH\",\n",
    "        )\n",
    "        ct = datetime.now()\n",
    "        print('Trying to retrieve predictions', ct)\n",
    "        model['predstore_future'] = level +  '_' + model['modelname'] + '_f' + str(FutureStart)\n",
    "        try:\n",
    "            predictions_future = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_future'])\n",
    "        except KeyError:\n",
    "            print(model['predstore_future'], ', run', run_id, 'does not exist, predicting')\n",
    "            predictions_future = model['RunResult_future'].run.future_point_predict(FutureStart,model['RunResult_future'].data)\n",
    "            predictions_future.forecasts.set_run(run_id)\n",
    "            predictions_future.forecasts.to_store(name=model['predstore_future'])  \n",
    "    model['algorithm'] = []\n",
    "    i = i + 1\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bb08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the future predictions\n",
    "\n",
    "\n",
    "predictions_test.xs(246,level=1).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b52249",
   "metadata": {},
   "source": [
    "## Notes on training time for the various algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are calculated in minutes for the hh20 feature set (with about 40 features), for all 36 steps, calibration (c) and test (t) partitions, also include generating predictions, and are approximate:\n",
    "\n",
    "#nj=12 (number of threads)\n",
    "#scikit random forest:        21:13 (c), 26:20 (t) RandomForestRegressor(n_estimators=200, n_jobs=nj)\n",
    "#XGB random forest:           06:02 (c), 07:51 (t) XGBRFRegressor(n_estimators=300,n_jobs=nj)\n",
    "#scikit gbm:                  13:59 (c), 15:55 (t) GradientBoostingRegressor(), \n",
    "#scikit hurdle random forest: 07:32 (c), 09:49 (t) For both clf and reg: (n_estimators=200, n_jobs=nj)\n",
    "#XGB hurdle xgb:              01:26 (c), 01:32 (t) For both clf and reg:                n_estimators=200,tree_method='hist',n_jobs=nj)\n",
    "#scikit histgbm:              01:17 (c), 01:20 (t) HistGradientBoostingRegressor(max_iter=200)\n",
    "#XGB xgb:                     01:00 (c), 01:04 (t) XGBRegressor(n_estimators=200,tree_method='hist',n_jobs=nj)\n",
    "#lightgbm gbm:                00:25 (c), --    (t) LGBMRegressor(n_estimators=100,num_threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71483a35",
   "metadata": {},
   "source": [
    "# Various helper functions and tools...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f053fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list | grep views-forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7570c6",
   "metadata": {},
   "source": [
    "# Retrieving external forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30211fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve David's Markov models\n",
    "# To do: rewrite the model dictionary to the new, slimmer version.\n",
    "DRList = []\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':   'fat_hh20_Markov_glm',\n",
    "    'algorithm': [],\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':      'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "}\n",
    "ModelList.append(model)\n",
    "DRList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':   'fat_hh20_Markov_rf',\n",
    "    'algorithm': [],\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':      'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "}\n",
    "ModelList.append(model)\n",
    "DRList.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "\n",
    "DRList[0]['predictions_file_calib'] = path + 'vmm_glm_hh20_0125_alt_calib.csv'\n",
    "DRList[0]['predictions_file_test'] = path + 'vmm_glm_hh20_0125_alt_test.csv'\n",
    "DRList[0]['predictions_file_future'] = path + 'vmm_glm_hh20_506.csv'\n",
    "\n",
    "DRList[1]['predictions_file_calib'] = path + 'vmm_rf_hh20_0125_alt_calib.csv'\n",
    "DRList[1]['predictions_file_test'] = path + 'vmm_rf_hh20_0125_alt_test.csv'\n",
    "DRList[1]['predictions_file_future'] = path + 'vmm_rf_hh20_505.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1e5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86478962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Markov models in central storage\n",
    "# Retrieving dependent variable\n",
    "target_calib = pd.DataFrame.forecasts.read_store('cm_fat_conflicthistory_rf_calib', run=run_id)['ln_ged_sb_dep']\n",
    "target_test = pd.DataFrame.forecasts.read_store('cm_fat_conflicthistory_rf_test', run=run_id)['ln_ged_sb_dep']\n",
    "level = 'cm'\n",
    "for model in DRList:\n",
    "    df_calib = pd.read_csv(model['predictions_file_calib'],index_col=['month_id','country_id'])\n",
    "    df_test = pd.read_csv(model['predictions_file_test'],index_col=['month_id','country_id'])\n",
    "    df_future = pd.read_csv(model['predictions_file_future'],index_col=['month_id','country_id'])\n",
    "    df_calib['ln_ged_sb_dep'] = target_calib\n",
    "    df_test['ln_ged_sb_dep'] = target_test\n",
    "    df_future['ln_ged_sb_dep'] = np.nan # Empty dependent variable column for consistency/required by prediction storage function\n",
    "    stored_modelname = level + '_' + model['modelname'] + '_calib'\n",
    "    df_calib.forecasts.set_run(run_id)\n",
    "    df_calib.forecasts.to_store(name=stored_modelname, overwrite=True)\n",
    "    stored_modelname = level + '_' + model['modelname'] + '_test'\n",
    "    df_test.forecasts.set_run(run_id)\n",
    "    df_test.forecasts.to_store(name=stored_modelname, overwrite=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing run result objects before saving\n",
    "# These should perhaps be discarded immediately above\n",
    "for model in ModelList[0:-3]:\n",
    "    model['RunResult_calib'] = []\n",
    "    model['RunResult_test'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f47c0f",
   "metadata": {},
   "source": [
    "# Save the model list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7493dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelList_df = pd.DataFrame.from_dict(ModelList)\n",
    "localpath = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/'\n",
    "\n",
    "filename = localpath + 'Model_cm_' + model['modelname'] + '_'+ dev_id + '.csv'\n",
    "ModelList_df.to_csv(filename)\n",
    "gitname = 'ModelList_cm_wide_' + dev_id + '.csv'\n",
    "ModelList_df.to_csv(gitname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
