{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c567d3bf",
   "metadata": {},
   "source": [
    "# Notebook to define ensemble for production, cm level\n",
    "Version developed for the UK FCDO\n",
    "## Including ensemble weighting\n",
    "\n",
    "This notebook defines the ensemble used for production: selects a set of pre-trained models, retrieves and calibrates them, computes weights, and computes and stores the ensemble model predictions.\n",
    "\n",
    "Models are stored in model storage and most of them specified in the notebook fat_cm_constituentmodels\n",
    "\n",
    "The notebook draws on the following files in this repository:\n",
    "\n",
    "Script file: \n",
    "    Ensembling.py\n",
    "    FetchData.py\n",
    "\n",
    "Lists of models:\n",
    "    ModelList_cm_{dev_id}.csv (not yet functional)\n",
    "    List of pickles at local directory (will rewrite to drop dependence on this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "from viewser import Queryset, Column\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "import views_dataviz\n",
    "from views_runs import storage, ModelMetadata\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "from views_forecasts.extensions import *\n",
    "# Other packages\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e8c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages from Predicting Fatalies repository\n",
    "\n",
    "from HurdleRegression import *\n",
    "from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "\n",
    "from FetchData import FetchData, RetrieveFromList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89992da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters:\n",
    "\n",
    "dev_id = 'Fatalities001'\n",
    "run_id = 'Fatalities001'\n",
    "EndOfHistory = 507\n",
    "RunGeneticAlgo = True\n",
    "level = 'cm'\n",
    "\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "\n",
    "fi_steps = [1,3,6,12,36]\n",
    "# Specifying partitions\n",
    "\n",
    "calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,444)}\n",
    "test_partitioner_dict = {\"train\":(121,444),\"predict\":(445,492)}\n",
    "future_partitioner_dict = {\"train\":(121,492),\"predict\":(493,504)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = '/Users/havardhegre/Dropbox (ViEWS)/ViEWS/'\n",
    "overleafpath = '/Users/havardhegre/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Tables/'\n",
    "localpath = '/Users/havardhegre/Pickles/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the ensemble\n",
    "# First item in dictionary is model name, second run id for development run\n",
    "\n",
    "ModelsToRead = ['fat_baseline_rf',\n",
    "                'fat_conflicthistory_srf',\n",
    "                'fat_conflicthistory_gbm',\n",
    "                'fat_conflicthistory_hurdle_lgb',\n",
    "                'fat_conflicthistory_long_xgb',\n",
    "                'fat_vdem_hurdle_xgb',\n",
    "                'fat_wdi_rf',\n",
    "                'fat_topics_rf',\n",
    "                'fat_topics_histgbm',\n",
    "                'fat_broad_xgb',\n",
    "                'fat_greatest_hits_hurdle_xgb',\n",
    "                'fat_hh20_hurdle_lgb',\n",
    "                'fat_all_pca3_xgb',\n",
    "                'fat_topics_pca3_lgb',\n",
    "                'fat_hh20_Markov_glm',\n",
    "                'fat_hh20_Markov_rf']\n",
    "\n",
    "ModelList = []\n",
    "\n",
    "# Read in list with models from model development notebook\n",
    "\n",
    "gitname = 'ModelList_cm_wide_' + dev_id + '.csv'\n",
    "ModelList_df = pd.read_csv(gitname)\n",
    "#ModelList_df.head(40)\n",
    "\n",
    "EnsembleMetaData = ModelList_df[ModelList_df['modelname'].isin(ModelsToRead)].copy()\n",
    "ModelList = EnsembleMetaData.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c159aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving revised model list in data frame form\n",
    "gitname = 'EnsembleMetaData_cm_' + dev_id + '.csv'\n",
    "EnsembleMetaData.to_csv(gitname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec1bfa",
   "metadata": {},
   "source": [
    "# Retrieve and calibrate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the predictions for calibration and test partitions\n",
    "# The ModelList contains the predictions organized by model\n",
    "\n",
    "ModelList = RetrieveStoredPredictions(ModelList, steps, EndOfHistory, run_id)\n",
    "\n",
    "ModelList = CalibratePredictions(ModelList, EndOfHistory, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81687a8e",
   "metadata": {},
   "source": [
    "# Genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed, cpu_count\n",
    "from functools import partial\n",
    "from genetic2 import *\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def make_run_from_step (\n",
    "    step,\n",
    "    e_set,\n",
    "    df_name = 'calib_df_calibrated', \n",
    "    target = 'ln_ged_sb_dep',\n",
    "    population_count = 100,\n",
    "    initial_population = None,\n",
    "    base_genes = np.array([0,1]),\n",
    "    number_of_generations = 500\n",
    "):\n",
    "    \"\"\"\n",
    "    step : step you want as an int,\n",
    "    ensemble_set : structure of the EnsembleList type,\n",
    "    target = Y in prediction,\n",
    "    df_name = name of the df in the ensemble set you want.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_step = f'step_pred_{step}'\n",
    "    \n",
    "    try: \n",
    "        del aggregate_df\n",
    "    except NameError:\n",
    "        pass \n",
    "    \n",
    "    for i_ens in ModelList:\n",
    "        try:\n",
    "            #Join the step from the model into the ensemble df if it exists.\n",
    "            aggregate_df = aggregate_df.join(i_ens[df_name][[df_step]], rsuffix=f'_{i_ens[\"modelname\"]}')\n",
    "        except NameError:\n",
    "            #If the ensemble df does not exist create it and include the target.\n",
    "            aggregate_df = i_ens[df_name][[target,df_step]].copy()\n",
    "            aggregate_df = aggregate_df.rename(columns = {df_step : f'{df_step}_{i_ens[\"modelname\"]}'})\n",
    "    \n",
    "    aggregate_df = aggregate_df.dropna()\n",
    "    aggregate_df = aggregate_df[aggregate_df.columns[~aggregate_df.columns.str.contains('ensemble')]]\n",
    "    \n",
    "    X = aggregate_df.copy(); del X[target]\n",
    "    Y = aggregate_df[target]\n",
    "    \n",
    "    inst_mse = partial(weighted_mse_score, Y, X, mean_squared_error)\n",
    "    if initial_population is None:\n",
    "        population =  init_population_sum(population_count,base_genes,X.shape[1],0.5,3)\n",
    "    else: \n",
    "        population = initial_population\n",
    "    \n",
    "    from genetic2 import temp_file_name\n",
    "    import os\n",
    "    Path('./exploration_pickle/').mkdir(parents=True, exist_ok=True) \n",
    "    pd.DataFrame({'step':[step], 'memoization_id':[temp_file_name]}).to_csv(f'exploration_pickle/id_{temp_file_name}.csv', index=False)\n",
    "    \n",
    "    generation = genetic_algorithm(population, \n",
    "                                   inst_mse, \n",
    "                                   base_genes, \n",
    "                                   f_thres=None, \n",
    "                                   ngen=number_of_generations, \n",
    "                                   pmut=0.2)\n",
    "    return {'step':step, 'memoization_id':temp_file_name, 'generation':generation}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_walrus_genes = np.array([0, 0.002, 0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.12, 0.14, 0.16, 0.18, 0.20, 0.25])\n",
    "nonlogged_genes = np.array([0, 0.02, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.9, 1, 1.1, 1.2, 1.5, 2.0])#, 1, 1.2, 1.4, 1.6, 1.8, 2.0, 2.5])\n",
    "print(len(nonlogged_genes))\n",
    "steps_to_optimize = [1,2,3,4,6,9,12,15,18,24,30,36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb862d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_function = partial(make_run_from_step, \n",
    "    e_set = ModelList,\n",
    "    df_name = 'predictions_calib_df', # Non-logged version\n",
    "    target = 'ln_ged_sb_dep',\n",
    "    population_count = 100,\n",
    "    initial_population = None,\n",
    "    base_genes = super_walrus_genes,\n",
    "    number_of_generations = 500\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b415a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = cpu_count()-4 if cpu_count()>2 else 1\n",
    "cpus - len(steps_to_optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 models, 24 genes, 12 steps, 100 generations takes 41 minutes\n",
    "if RunGeneticAlgo:\n",
    "\n",
    "    ct = datetime.now()\n",
    "    print('Estimating genetic weights, current time:', ct)\n",
    "    generations = Parallel(n_jobs=cpus)(delayed(filled_function)(i) for i in steps_to_optimize)\n",
    "    ct = datetime.now()\n",
    "    print('Done estimating weights, current time:', ct)\n",
    "    with open('exploration_pickle/full_gen.pickle', 'wb') as handle:\n",
    "        pkl.dump(generations, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    picklename = 'exploration_pickle/full_gen.pickle'\n",
    "    generations = pkl.load( open (picklename, \"rb\") )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac54121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the memoization id's so that you can explore the training process in the visualizer\n",
    "for i in generations:\n",
    "    print (i['step'], i['memoization_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the best organism.\n",
    "GeneticAlgoResult = []\n",
    "for gen in generations:\n",
    "    print ('\\nStep: ',gen['step'],'\\n','*'*24,'\\n')\n",
    "    print (gen['generation'][0])\n",
    "    #The best is always the top organism. You can get the top 20 by slicing gen['generation'][0:20] and so on\n",
    "    linedict = {\n",
    "        'Org': gen['generation'][0][0],\n",
    "        'Fitness': gen['generation'][0][1],\n",
    "        'Step': gen['step']\n",
    "    }\n",
    "    GeneticAlgoResult.append(linedict)\n",
    "print(GeneticAlgoResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475df80d",
   "metadata": {},
   "source": [
    "# Assignment of the genetic weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from GeneticAlgoResult:\n",
    "w_step = [None] * 37\n",
    "for line in GeneticAlgoResult:\n",
    "    w_step[line['Step']] = line['Org']\n",
    "i=2\n",
    "for i in [1,2,3,6,9,12,18,24,30,36]:\n",
    "    w_step[i]\n",
    "    print(sum(w_step[i]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear interpolation of weights:\n",
    "print(steps_to_optimize)\n",
    "WeightMatrix = [None] * 37\n",
    "modelnames = []\n",
    "for model in ModelList: \n",
    "    modelnames.append(model['modelname'])\n",
    "for step in steps:\n",
    "    if step in steps_to_optimize:\n",
    "#        print(step, 'is optimized')\n",
    "        WeightMatrix[step] = w_step[step]\n",
    "    else:\n",
    "        WeightMatrix[step] = np.nan * len(w_step[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f72bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "StepAssigner = [1,2,3,4,4,6,6,9,9,9,12,12,12,15,15,15,18,18,18,18,18,24,24,24,24,24,24,30,30,30,30,30,30,36,36,36]\n",
    "WeightMatrix = [None] * 37\n",
    "\n",
    "stepcols = ['ln_ged_sb_dep']\n",
    "for step in steps:\n",
    "    stepcols.append('step_pred_' + str(step))\n",
    "modelnames = []\n",
    "for model in ModelList: \n",
    "    modelnames.append(model['modelname'])\n",
    "\n",
    "for step in steps:\n",
    "#    print('Step',step,'assigned',StepAssigner[step-1])\n",
    "    WeightMatrix[step] = w_step[StepAssigner[step-1]]\n",
    "wmt = np.array(WeightMatrix[1:]).T\n",
    "weights_df = pd.DataFrame(wmt,columns=stepcols[1:],index=modelnames)\n",
    "weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolated weights\n",
    "i_weights_df = weights_df.copy()\n",
    "for step in steps:\n",
    "    col = 'step_pred_' + str(step)\n",
    "    if step == 5:\n",
    "        prestepcol = 'step_pred_' + str(step-1)\n",
    "        \n",
    "        poststepcol = 'step_pred_' + str(step+1)\n",
    "        i_weights_df[col] = (i_weights_df[prestepcol] + i_weights_df[poststepcol]) / 2\n",
    "    if step == 7 or step == 10 or step == 13 or step == 16:\n",
    "        prestepcol = 'step_pred_' + str(step-1)\n",
    "        poststepcol = 'step_pred_' + str(step+2)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*2) + (i_weights_df[poststepcol]*1)) / 3\n",
    "    if step == 8 or step == 11 or step == 14 or step == 17:\n",
    "        prestepcol = 'step_pred_' + str(step-2)\n",
    "        poststepcol = 'step_pred_' + str(step+1)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*1) + (i_weights_df[poststepcol]*2)) / 3\n",
    "    if step == 19 or step == 25 or step == 31:\n",
    "        prestepcol = 'step_pred_' + str(step-1)\n",
    "        poststepcol = 'step_pred_' + str(step+5)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*5) + (i_weights_df[poststepcol]*1)) / 6\n",
    "    if step == 20 or step == 26 or step == 32:\n",
    "        prestepcol = 'step_pred_' + str(step-2)\n",
    "        poststepcol = 'step_pred_' + str(step+3)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*4) + (i_weights_df[poststepcol]*2)) / 6\n",
    "    if step == 21 or step == 27 or step == 33:\n",
    "        prestepcol = 'step_pred_' + str(step-3)\n",
    "        poststepcol = 'step_pred_' + str(step+3)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*3) + (i_weights_df[poststepcol]*3)) / 6\n",
    "    if step == 22 or step == 28 or step == 34:\n",
    "        prestepcol = 'step_pred_' + str(step-4)\n",
    "        poststepcol = 'step_pred_' + str(step+2)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*2) + (i_weights_df[poststepcol]*4)) / 6\n",
    "    if step == 23 or step == 29 or step == 35:\n",
    "        prestepcol = 'step_pred_' + str(step-5)\n",
    "        poststepcol = 'step_pred_' + str(step+1)\n",
    "        i_weights_df[col] = ((i_weights_df[prestepcol]*1) + (i_weights_df[poststepcol]*5)) / 6\n",
    "        \n",
    "print(steps_to_optimize)\n",
    "# Export weights \n",
    "i_weights_df.to_csv('GeneticWeights.csv')\n",
    "i_weights_df\n",
    "# Save the weights dfs\n",
    "dflist = [\n",
    "    (i_weights_df,'i_weights_df'), \n",
    "]\n",
    "\n",
    "path = Mydropbox + 'Projects/PredictingFatalities/MSEs/'\n",
    "for df in dflist:\n",
    "    filename = path + df[1] + '.csv'\n",
    "    df[0].to_csv(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "palette = 'vlag'\n",
    "palette = sns.color_palette('BrBG',n_colors=50)\n",
    "palette = sns.cubehelix_palette(start=2, rot=0, dark=0, light=1, n_colors=100)\n",
    "\n",
    "fig, ax =plt.subplots(1,figsize=(16,11))\n",
    "ax = sns.heatmap(i_weights_df, xticklabels=2, linewidths=.5, cmap=palette,square=True)\n",
    "filename = overleafpath + 'genetic_weights.png'\n",
    "plt.savefig(filename, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing dfs to hold the predictions\n",
    "# A list of dictionaries organizing predictions and information as one step per entry,\n",
    "# including a dataframe for each step with one column per prediction model\n",
    "StepEnsembles = []\n",
    "for col in stepcols[1:]:  # Use the baseline as template to construct object\n",
    "    Step_prediction = {\n",
    "        'step_pred': col,\n",
    "        'df_calib': pd.DataFrame(ModelList[0]['calib_df_calibrated']['ln_ged_sb_dep']), \n",
    "        'df_test': pd.DataFrame(ModelList[0]['test_df_calibrated']['ln_ged_sb_dep']),\n",
    "        'ensembles_calib': pd.DataFrame(ModelList[0]['calib_df_calibrated']['ln_ged_sb_dep']),\n",
    "        'ensembles_test': pd.DataFrame(ModelList[0]['test_df_calibrated']['ln_ged_sb_dep'])\n",
    "    }\n",
    "    for model in ModelList:\n",
    "        modelname = model['modelname']\n",
    "        Step_prediction['df_calib'][modelname] = model['calib_df_calibrated'][col]\n",
    "        Step_prediction['df_test'][modelname] = model['test_df_calibrated'][col]\n",
    "    StepEnsembles.append(Step_prediction)\n",
    "\n",
    "# Calculating unweighted average ensembles\n",
    "i = 0\n",
    "for col in stepcols[1:]:\n",
    "    # Unweighted average\n",
    "    StepEnsembles[i]['ensembles_test']['unweighted_average'] = StepEnsembles[i]['df_test'].drop('ln_ged_sb_dep', axis=1).mean(axis=1)\n",
    "    StepEnsembles[i]['ensembles_calib'].loc['unweighted_average'] = StepEnsembles[i]['df_calib'].drop('ln_ged_sb_dep', axis=1).mean(axis=1)\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10128a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "StepEnsembles[0]['ensembles_test']['unweighted_average']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ad53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating weighted average ensembles\n",
    "# Based on the weights_df dataframe filled with Mihai's weights above\n",
    "\n",
    "def ensemble_predictions(yhats, weights):\n",
    "    # make predictions\n",
    "    yhats = np.array(yhats)\n",
    "    # weighted sum across ensemble members\n",
    "    result = np.dot(weights,yhats)\n",
    "    return result\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "i = 0\n",
    "for col in stepcols[1:]:\n",
    "    # Unweighted average\n",
    "    df_calib = StepEnsembles[i]['df_calib'].drop('ln_ged_sb_dep', axis=1)\n",
    "    df_test = StepEnsembles[i]['df_test'].drop('ln_ged_sb_dep', axis=1)\n",
    "    StepEnsembles[i]['ensembles_calib']['weighted_average'] = (df_calib*i_weights_df[col]).sum(axis=1)\n",
    "    StepEnsembles[i]['ensembles_test']['weighted_average'] =  (df_test*i_weights_df[col]).sum(axis=1)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a99d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the ensemble predictions\n",
    "EnsembleList = []\n",
    "genetic = {\n",
    "        'modelname': 'ensemble_genetic',\n",
    "        'algorithm': '',\n",
    "        'depvar': \"ln_ged_sb_dep\",\n",
    "        'calib_df_calibrated': ModelList[0]['calib_df_calibrated'].copy(),\n",
    "        'test_df_calibrated': ModelList[0]['test_df_calibrated'].copy(),\n",
    "    }    \n",
    "\n",
    "for step in StepEnsembles:\n",
    "    colname = step['step_pred']\n",
    "    print(colname)\n",
    "    genetic['calib_df_calibrated'][colname] = step['ensembles_calib']['weighted_average']\n",
    "    genetic['test_df_calibrated'][colname] = step['ensembles_test']['weighted_average']\n",
    "\n",
    "EnsembleList.append(genetic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daadab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble predictions\n",
    "predstore_calib = level +  '_' + genetic['modelname'] + '_calib'\n",
    "genetic['calib_df_calibrated'].forecasts.set_run(run_id)\n",
    "genetic['calib_df_calibrated'].forecasts.to_store(name=predstore_calib, overwrite = True)\n",
    "predstore_test = level +  '_' + genetic['modelname'] + '_test'\n",
    "genetic['test_df_calibrated'].forecasts.set_run(run_id)\n",
    "genetic['test_df_calibrated'].forecasts.to_store(name=predstore_test, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305072e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See which genetic ensembles are in prediction storage\n",
    "ViewsMetadata().with_name('genetic').fetch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
